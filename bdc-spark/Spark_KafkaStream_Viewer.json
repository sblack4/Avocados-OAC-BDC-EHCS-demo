{"paragraphs":[{"text":"%md\n\n# Spark Streaming ","user":"anonymous","dateUpdated":"2018-03-13T16:32:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Streaming</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1519662992837_994952686","id":"20180226-163632_374321565","dateCreated":"2018-02-26T16:36:32+0000","dateStarted":"2018-03-13T16:32:42+0000","dateFinished":"2018-03-13T16:32:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:61231"},{"title":"Use Angular to bind Event Hub Connector","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n//save these for pyspark\nz.put(\"BIND_OEHCS_Topic\", z.angular(\"BIND_OEHCS_Topic\"))\nz.put(\"BIND_OEHCS_ConnectionDescriptor\", z.angular(\"BIND_OEHCS_ConnectionDescriptor\"))\n\n//save these for shell\nscala.tools.nsc.io.File(\"/var/lib/zeppelin/oehcs.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","user":"anonymous","dateUpdated":"2018-03-13T15:18:23+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"editorHide":true},"settings":{"params":{"OEHCS_ConnectionDescriptor":"129.157.179.70:6667","OEHCS_Topic":"idcs-399b050795c64e8db9298cafe60966e9-Pandora"},"forms":{"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false,"$$hashKey":"object:61542"},"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false,"$$hashKey":"object:61543"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"done\n"}]},"apps":[],"jobName":"paragraph_1519663012288_958032116","id":"20180226-163652_201978658","dateCreated":"2018-02-26T16:36:52+0000","dateStarted":"2018-02-26T16:44:22+0000","dateFinished":"2018-02-26T16:44:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61232"},{"text":"%md\n\n# Get Raw Tweets from Kafka ","user":"anonymous","dateUpdated":"2018-02-26T16:41:08+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Get Raw Tweets from Kafka</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1519663056647_-568758789","id":"20180226-163736_1435072909","dateCreated":"2018-02-26T16:37:36+0000","dateStarted":"2018-02-26T16:41:08+0000","dateFinished":"2018-02-26T16:41:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61233"},{"text":"%spark\nimport org.apache.spark.sql.Dataset\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types\nimport scala.util.Random\nimport java.time.format.DateTimeFormatter\nimport java.time.LocalDate\n\n\ncase class User(\n    id: Long, \n    name: String,\n    description: String, \n    followers_count: Long, \n    location: String,\n    friends_count: Long, \n    screen_name: String\n)\n\n\ncase class Tweet(\n    id: Long,\n    created_at: String,         \n    text: String,\n    favorite_count: Int,\n    retweet_count: Int,\n    quote_count: Int,\n    reply_count: Int,\n    lang: String,\n    coordinates: String,\n    place: String,\n    possibly_sensitive: String,\n    user: User\n)\n\n\n/**\n * Parses dates (created_at from twitter) of format: \"Thu Feb 15 22:00:54 +0000 2018\",\n * this method doesn't allow for use as UDF\n */\ndef parseDate(dateStr: String): LocalDate = {\n    val dateFormat = DateTimeFormatter.ofPattern(\"c L d H:m:s Z y\")\n    try {\n        return LocalDate.parse(dateStr, dateFormat)\n    } catch {\n        case e:Exception=> return LocalDate.now()\n    }\n}\n    \ndef randState(): String = {\n    val states = List(\"Utah\", \"Hawaii\", \"Minnesota\", \"Ohio\", \"Arkansas\", \"Oregon\", \"Texas\", \n        \"North Dakota\", \"Pennsylvania\", \"Connecticut\", \"Nebraska\", \"Vermont\", \"Nevada\", \n        \"Washington\", \"Illinois\", \"Oklahoma\", \"District of Columbia\", \"Delaware\", \n        \"Alaska\", \"New Mexico\", \"West Virginia\", \"Missouri\", \"Rhode Island\", \"Georgia\", \n        \"Montana\", \"Michigan\", \"Virginia\", \"North Carolina\", \"Wyoming\", \"Kansas\", \n        \"New Jersey\", \"Maryland\", \"Alabama\", \"Arizona\", \"Iowa\", \"Massachusetts\", \n        \"Kentucky\", \"Louisiana\", \"Mississippi\", \"New Hampshire\", \"Tennessee\", \n        \"Florida\", \"Indiana\", \"Idaho\", \"South Carolina\", \"South Dakota\", \"California\", \n        \"New York\", \"Wisconsin\", \"Colorado\", \"Maine\"\n    )\n    return states(Random.nextInt(states.size))\n}\n\ndef randProvider():String = {\n    return List(\"A\", \"B\", \"C\")(Random.nextInt(3))\n}\n\ndef handleJson(df: Dataset[Tweet]) = {\n    val filtered_df = df\n        .filter(\"possibly_sensitive = false\")\n        .map(row => Row(\n            row.id,\n            row.text,\n            row.favorite_count,\n            row.retweet_count,\n            row.quote_count,\n            row.reply_count,\n            row.lang,\n            row.user.id,\n            parseDate(row.created_at).alias(\"datetime\"),\n            randState().alias(\"state\"),\n            randProvider().alias(\"provider\")\n        )\n    )\n    \n    val filtered_df = df.select(\n        col(\"id\"),\n        expr(\"COALESCE(text, \\\"null\\\") AS text\"),\n        expr(\"COALESCE(favorite_count, 0) AS favorite_count\"),\n        expr(\"COALESCE(retweet_count, 0) AS retweet_count\"),\n        expr(\"COALESCE(quote_count, 0) AS quote_count\"),\n        expr(\"COALESCE(reply_count, 0) as reply_count\"),\n        expr(\"COALESCE(lang, \\\"und\\\") as lang\"),\n        expr(\"COALESCE(coordinates, 0) as coordinates\"),\n        expr(\"COALESCE(place, \\\"null\\\") as place\"),\n        col(\"user.id\").alias(\"user_id\"),\n        expr(\"datetime\"),\n        expr(\"state\"),\n        expr(\"provider\")\n    )\n    tweets.write.mode(\"append\").insertInto(\"default.tweets\")\n    \n    val users = df.select(\n        \"user.id\",\n        \"user.name\",\n        \"user.description\",\n        \"user.followers_count\",\n        \"user.location\",\n        \"user.friends_count\",\n        \"user.screen_name\"\n    )\n    users.write.mode(\"append\").insertInto(\"default.users\")\n}","user":"anonymous","dateUpdated":"2018-03-13T19:01:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[]},"apps":[],"jobName":"paragraph_1520954329513_88860221","id":"20180313-151849_1815330875","dateCreated":"2018-03-13T15:18:49+0000","dateStarted":"2018-03-13T19:01:52+0000","dateFinished":"2018-03-13T19:01:57+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:61234"},{"title":"Consume Streaming Data","text":"%spark\n{\nimport _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\n\n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(5))\n \n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n \n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\n\n\n println(\"Setting up operations on DStream\")    \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n messages.foreachRDD(rdd => {\n    var df=sqlContext.read.json(rdd.map(x => x._2))\n    df.show()\n    // var df=sqlContext.createDataFrame(rdd)\n    println(df.getClass.getSimpleName)\n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n println(\"Will now sleep for a few minutes, before stopping the StreamingContext.  At this point, you should start the producer.\")\n Thread.sleep(90)//000)  //now sleep for 1.5 minutes.  Parameter is milliseconds\n\n //stop any active streamingcontexts.  Parameters are boolean stopSparkContext, boolean stopGracefully\n println(\"Stopping Active StreamingContext\")\n StreamingContext.getActive().map(_.stop(false,true))\n println(\"done\")\n}","user":"anonymous","dateUpdated":"2018-03-13T15:38:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Creating new Streaming Context\ntopic:idcs-399b050795c64e8db9298cafe60966e9-Pandora\nbrokers:129.157.179.70:6667\nCreating Kafka DStream\nSetting up operations on DStream\nStarting Streaming Context\nWill now sleep for a few minutes, before stopping the StreamingContext.  At this point, you should start the producer.\nStopping Active StreamingContext\n++\n||\n++\n++\n\nDataset\n++\n||\n++\n++\n\nDataset\ndone\n"}]},"apps":[],"jobName":"paragraph_1519663030137_-1700567460","id":"20180226-163710_430219123","dateCreated":"2018-02-26T16:37:10+0000","dateStarted":"2018-03-13T15:38:50+0000","dateFinished":"2018-03-13T15:39:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61235"},{"title":"Stop all Spark Streaming","text":"%spark\n{\n    import org.apache.spark.streaming._\n    println(\"Stopping any active StreamingContext.  May take a minute.\")\n    StreamingContext.getActive().map(_.stop(false,true))\n    println(\"done\")\n}","user":"anonymous","dateUpdated":"2018-02-26T16:50:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Stopping any active StreamingContext.  May take a minute.\ndone\n"}]},"apps":[],"jobName":"paragraph_1519663121318_-692355131","id":"20180226-163841_1704805750","dateCreated":"2018-02-26T16:38:41+0000","dateStarted":"2018-02-26T16:50:24+0000","dateFinished":"2018-02-26T16:51:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61236"},{"text":"%md \n\n## Let's inspect our data\n","user":"anonymous","dateUpdated":"2018-03-13T15:27:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Let&rsquo;s inspect our data</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1519663824093_1900924437","id":"20180226-165024_786375792","dateCreated":"2018-02-26T16:50:24+0000","dateStarted":"2018-03-13T15:27:12+0000","dateFinished":"2018-03-13T15:27:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61237"},{"text":"%spark \n\n// class structure \ncase class State(state: String, number_of_tweets:Int)\n\n// Unbind angular variable in case it already exists from previous run\nz.angularUnbind(\"topstates\") \n\n// Define a new dataframe based off a query\nval topstatesdf = sqlContext.sql(\"\"\"\nSELECT \n    count(*) as number_of_tweets, \n    state\nFROM tweets\nWHERE state is not null\nGROUP BY state\n\"\"\")\n\n\n// Map the DF into an Array of Stations\nval state_array = topstatesdf.map(x => State(x.get(1).toString, x.get(0).toString.toInt)).collect\n\n// Bind the TopStations (as an Array) to an Angular variable named topstations\nz.angularBind(\"topstates\", state_array) \n\nprintln(\"..\")\nprintln(\"done\")\n","user":"anonymous","dateUpdated":"2018-02-27T21:35:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class State\n\ntopstatesdf: org.apache.spark.sql.DataFrame = [number_of_tweets: bigint, state: string]\n\nstate_array: Array[State] = Array(State(Utah,152), State(Hawaii,8), State(Ohio,106), State(Texas,215), State(North Dakota,2), State(Pennsylvania,22), State(Connecticut,4366), State(Nebraska,2), State(Nevada,11), State(Washington,166), State(Illinois,2), State(Oklahoma,57), State(Alaska,28), State(New Mexico,54), State(Missouri,2), State(Rhode Island,984), State(Michigan,2), State(Kansas,3), State(Maryland,19), State(Alabama,10), State(Arizona,1391), State(Iowa,1950), State(Kentucky,1), State(Louisiana,53), State(Mississippi,336), State(Tennessee,2), State(Florida,7193), State(Indiana,2), State(California,2), State(New York,1), State(Colorado,45), State(Maine,2))\n..\ndone\n"}]},"apps":[],"jobName":"paragraph_1519767177690_-242781376","id":"20180227-213257_891636805","dateCreated":"2018-02-27T21:32:57+0000","dateStarted":"2018-02-27T21:35:01+0000","dateFinished":"2018-02-27T21:35:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61238"},{"text":"%angular\n\n<p>{{topstates}}</p>\n\n","user":"anonymous","dateUpdated":"2018-02-27T21:35:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":"true"},"editorMode":"ace/mode/undefined","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<p>{{topstates}}</p>"}]},"apps":[],"jobName":"paragraph_1519767181784_1332380621","id":"20180227-213301_565519281","dateCreated":"2018-02-27T21:33:01+0000","dateStarted":"2018-02-27T21:35:28+0000","dateFinished":"2018-02-27T21:35:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61239"},{"text":"%angular\n","user":"anonymous","dateUpdated":"2018-02-27T21:35:16+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":"true"},"editorMode":"ace/mode/undefined"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519767316288_-2074206922","id":"20180227-213516_78267030","dateCreated":"2018-02-27T21:35:16+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:61240"}],"name":"Stream Viewer","id":"2D8X3Z4SW","angularObjects":{"2D62F3PCD:shared_process":[],"2D94CA3FA:shared_process":[],"2D61W4KDW:shared_process":[],"2D9TNN93P:shared_process":[],"2D6RURC6K:shared_process":[],"2D6HF4GAX:shared_process":[],"2C4U48MY3_spark2:shared_process":[{"name":"BIND_ObjectStorage_Container","object":"","noteId":"2D8X3Z4SW"},{"name":"topstates","object":[{"state":"Utah","number_of_tweets":152},{"state":"Hawaii","number_of_tweets":8},{"state":"Ohio","number_of_tweets":106},{"state":"Texas","number_of_tweets":215},{"state":"North Dakota","number_of_tweets":2},{"state":"Pennsylvania","number_of_tweets":22},{"state":"Connecticut","number_of_tweets":4366},{"state":"Nebraska","number_of_tweets":2},{"state":"Nevada","number_of_tweets":11},{"state":"Washington","number_of_tweets":166},{"state":"Illinois","number_of_tweets":2},{"state":"Oklahoma","number_of_tweets":57},{"state":"Alaska","number_of_tweets":28},{"state":"New Mexico","number_of_tweets":54},{"state":"Missouri","number_of_tweets":2},{"state":"Rhode Island","number_of_tweets":984},{"state":"Michigan","number_of_tweets":2},{"state":"Kansas","number_of_tweets":3},{"state":"Maryland","number_of_tweets":19},{"state":"Alabama","number_of_tweets":10},{"state":"Arizona","number_of_tweets":1391},{"state":"Iowa","number_of_tweets":1950},{"state":"Kentucky","number_of_tweets":1},{"state":"Louisiana","number_of_tweets":53},{"state":"Mississippi","number_of_tweets":336},{"state":"Tennessee","number_of_tweets":2},{"state":"Florida","number_of_tweets":7193},{"state":"Indiana","number_of_tweets":2},{"state":"California","number_of_tweets":2},{"state":"New York","number_of_tweets":1},{"state":"Colorado","number_of_tweets":45},{"state":"Maine","number_of_tweets":2}],"noteId":"2D8X3Z4SW"},{"name":"BIND_OEHCS_ConnectionDescriptor","object":"129.157.179.70:6667","noteId":"2D8X3Z4SW"},{"name":"BIND_OEHCS_Topic","object":"idcs-399b050795c64e8db9298cafe60966e9-Pandora","noteId":"2D8X3Z4SW"}],"2D7CTVNXY:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}